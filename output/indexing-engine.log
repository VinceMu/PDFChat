13:30:54,193 graphrag.index.cli INFO Logging enabled at /Users/vincemu/Development/FAQ_bot/output/indexing-engine.log
13:30:54,196 graphrag.index.cli INFO Starting pipeline run for: 20241009-133054, dryrun=False
13:30:54,197 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4-turbo-preview",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 0,
        "requests_per_minute": 0,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "/Users/vincemu/Development/FAQ_bot",
    "reporting": {
        "type": "file",
        "base_dir": "/Users/vincemu/Development/FAQ_bot/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/Users/vincemu/Development/FAQ_bot/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "knowledge_base",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 600,
        "overlap": 50,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4-turbo-preview",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4-turbo-preview",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4-turbo-preview",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4-turbo-preview",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": true,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
13:30:54,199 graphrag.index.create_pipeline_config INFO skipping workflows 
13:30:54,200 graphrag.index.run.run INFO Running pipeline
13:30:54,200 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /Users/vincemu/Development/FAQ_bot/output
13:30:54,200 graphrag.index.input.load_input INFO loading input from root_dir=knowledge_base
13:30:54,200 graphrag.index.input.load_input INFO using file storage for input
13:30:54,201 graphrag.index.storage.file_pipeline_storage INFO search /Users/vincemu/Development/FAQ_bot/knowledge_base for files matching .*\.txt$
13:30:54,202 graphrag.index.input.text INFO found text files from knowledge_base, found [('PepsiCo-Global-Human-Rights-Policy.txt', {})]
13:30:54,204 graphrag.index.input.text INFO Found 1 files, loading 1
13:30:54,205 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_final_covariates', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
13:30:54,205 graphrag.index.run.run INFO Final # of rows loaded: 1
13:30:54,338 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
13:30:54,343 datashaper.workflow.workflow INFO executing verb orderby
13:30:54,347 datashaper.workflow.workflow INFO executing verb zip
13:30:54,351 datashaper.workflow.workflow INFO executing verb aggregate_override
13:30:54,357 datashaper.workflow.workflow INFO executing verb chunk
13:30:54,540 datashaper.workflow.workflow INFO executing verb select
13:30:54,544 datashaper.workflow.workflow INFO executing verb unroll
13:30:54,550 datashaper.workflow.workflow INFO executing verb rename
13:30:54,555 datashaper.workflow.workflow INFO executing verb genid
13:30:54,560 datashaper.workflow.workflow INFO executing verb unzip
13:30:54,567 datashaper.workflow.workflow INFO executing verb copy
13:30:54,573 datashaper.workflow.workflow INFO executing verb filter
13:30:54,586 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
13:30:54,741 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
13:30:54,741 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
13:30:54,758 datashaper.workflow.workflow INFO executing verb entity_extract
13:30:54,762 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
13:30:54,772 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4-turbo-preview: TPM=0, RPM=0
13:30:54,772 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4-turbo-preview: 25
13:30:55,116 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:30:55,119 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:30:55,135 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:30:55,138 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:30:55,140 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:30:55,142 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:30:56,460 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:30:56,462 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:30:56,932 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:30:56,934 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:30:57,98 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:30:57,101 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:30:59,253 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:30:59,255 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:30:59,436 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:30:59,438 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:30:59,941 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:30:59,943 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:31:03,934 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:31:03,936 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:31:04,506 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:31:04,508 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:31:04,740 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:31:04,742 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:31:13,347 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:31:13,350 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:31:13,570 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:31:13,573 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:31:16,640 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:31:16,646 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 21.85357137599931. input_tokens=2587, output_tokens=568
13:31:19,502 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:31:19,507 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 24.727203271999315. input_tokens=2586, output_tokens=424
13:31:19,807 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:31:19,809 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:31:21,161 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:31:21,163 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:31:23,845 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:31:23,847 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:31:23,936 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:31:23,938 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:31:26,259 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:31:26,263 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 9.613491997999517. input_tokens=34, output_tokens=218
13:31:30,459 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:31:30,465 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 35.667044216999784. input_tokens=2587, output_tokens=998
13:31:30,502 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:31:30,507 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 35.716476465000596. input_tokens=2582, output_tokens=1005
13:31:30,747 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:31:30,749 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:31:32,98 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:31:32,102 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 4 retries took 19.198268909000035. input_tokens=2587, output_tokens=489
13:31:32,402 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:31:32,405 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:31:32,813 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:31:32,815 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:31:34,82 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:31:34,84 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:31:34,438 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:31:34,440 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:31:35,281 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:31:35,283 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:31:37,831 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:31:37,835 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 7.365506314999948. input_tokens=34, output_tokens=188
13:31:40,362 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:31:40,364 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:31:41,310 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:31:41,314 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 3 retries took 12.603356612000425. input_tokens=34, output_tokens=347
13:31:44,795 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:31:44,800 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 5 retries took 21.446981478999078. input_tokens=2587, output_tokens=603
13:31:49,434 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:31:49,437 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:31:57,757 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:31:57,765 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 7 retries took 13.677190692000295. input_tokens=2191, output_tokens=304
13:31:59,733 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:31:59,736 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:32:00,462 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:00,467 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 23.695202959999733. input_tokens=34, output_tokens=647
13:32:07,528 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:07,533 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 9.762950725998962. input_tokens=34, output_tokens=269
13:32:14,401 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:14,406 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 29.599728934999803. input_tokens=34, output_tokens=890
13:32:18,72 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:18,77 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 6 retries took 8.337356059999365. input_tokens=34, output_tokens=223
13:32:18,104 datashaper.workflow.workflow INFO executing verb merge_graphs
13:32:18,116 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
13:32:18,265 graphrag.index.run.workflow INFO dependencies for create_final_covariates: ['create_base_text_units']
13:32:18,266 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
13:32:18,281 datashaper.workflow.workflow INFO executing verb extract_covariates
13:32:18,605 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:32:18,607 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:32:18,609 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:32:18,610 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:32:18,612 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:32:18,613 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:32:20,214 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:32:20,216 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:32:20,316 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:32:20,319 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:32:20,409 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:32:20,411 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:32:22,707 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:32:22,708 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:32:22,860 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:32:22,862 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:32:22,950 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:32:22,952 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:32:27,93 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:32:27,95 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:32:27,435 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:32:27,437 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:32:34,972 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:34,976 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 16.66727291799907. input_tokens=1321, output_tokens=387
13:32:35,522 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:32:35,539 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:32:36,252 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:32:36,253 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:32:38,931 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:38,935 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 20.640929488999973. input_tokens=1714, output_tokens=587
13:32:39,443 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:39,450 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 21.16010292899955. input_tokens=1713, output_tokens=628
13:32:39,662 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:32:39,664 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:32:40,884 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:40,886 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 22.583645463999346. input_tokens=1715, output_tokens=597
13:32:41,96 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:32:41,99 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:32:43,272 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:32:43,273 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:32:43,462 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:43,466 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 8.48587897499965. input_tokens=19, output_tokens=213
13:32:45,796 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:32:45,806 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:32:46,479 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:32:46,480 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:32:48,102 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:48,106 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 3 retries took 20.16927469300026. input_tokens=1715, output_tokens=591
13:32:48,330 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:32:48,331 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:32:49,264 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:32:49,268 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 10.328502718999516. input_tokens=19, output_tokens=276
13:32:56,729 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:32:56,732 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:33:09,790 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:09,795 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 24.349958116999915. input_tokens=19, output_tokens=714
13:33:13,272 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:13,277 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 23.878281584000433. input_tokens=19, output_tokens=676
13:33:21,57 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:21,64 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 6 retries took 25.253749958999833. input_tokens=1715, output_tokens=721
13:33:27,917 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:27,923 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 47.053526044001046. input_tokens=19, output_tokens=1226
13:33:40,513 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:40,519 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 7 retries took 33.78293664099874. input_tokens=1715, output_tokens=798
13:33:42,252 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:42,258 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 21.187069752999378. input_tokens=19, output_tokens=591
13:33:50,754 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:50,759 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 10.235676512000282. input_tokens=19, output_tokens=275
13:33:50,788 datashaper.workflow.workflow INFO executing verb window
13:33:50,797 datashaper.workflow.workflow INFO executing verb genid
13:33:50,806 datashaper.workflow.workflow INFO executing verb convert
13:33:50,824 datashaper.workflow.workflow INFO executing verb rename
13:33:50,833 datashaper.workflow.workflow INFO executing verb select
13:33:50,836 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_covariates.parquet
13:33:51,19 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
13:33:51,21 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
13:33:51,40 datashaper.workflow.workflow INFO executing verb summarize_descriptions
13:33:54,746 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:54,750 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.6861439450003672. input_tokens=311, output_tokens=84
13:33:54,807 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:54,811 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.727799311000126. input_tokens=268, output_tokens=77
13:33:54,814 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:54,817 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.726567760000762. input_tokens=318, output_tokens=92
13:33:55,874 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:55,879 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.807743302000745. input_tokens=322, output_tokens=113
13:33:55,901 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:55,904 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.827631247999307. input_tokens=306, output_tokens=119
13:33:56,79 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:56,84 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.016076890999102. input_tokens=322, output_tokens=92
13:33:56,489 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:56,494 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.413477422998767. input_tokens=322, output_tokens=103
13:33:57,635 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:57,640 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.5466588489998685. input_tokens=314, output_tokens=138
13:33:58,992 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:33:58,997 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.910592399999587. input_tokens=336, output_tokens=180
13:33:59,27 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
13:33:59,187 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
13:33:59,188 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
13:33:59,210 datashaper.workflow.workflow INFO executing verb cluster_graph
13:33:59,251 datashaper.workflow.workflow INFO executing verb select
13:33:59,254 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
13:33:59,409 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
13:33:59,410 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
13:33:59,433 datashaper.workflow.workflow INFO executing verb unpack_graph
13:33:59,449 datashaper.workflow.workflow INFO executing verb rename
13:33:59,460 datashaper.workflow.workflow INFO executing verb select
13:33:59,471 datashaper.workflow.workflow INFO executing verb dedupe
13:33:59,494 datashaper.workflow.workflow INFO executing verb rename
13:33:59,508 datashaper.workflow.workflow INFO executing verb filter
13:33:59,537 datashaper.workflow.workflow INFO executing verb text_split
13:33:59,556 datashaper.workflow.workflow INFO executing verb drop
13:33:59,571 datashaper.workflow.workflow INFO executing verb merge
13:33:59,600 datashaper.workflow.workflow INFO executing verb text_embed
13:33:59,601 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
13:33:59,608 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-3-small: TPM=0, RPM=0
13:33:59,608 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-3-small: 25
13:33:59,615 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 57 inputs via 57 snippets using 4 batches. max_batch_size=16, max_tokens=8191
13:34:00,73 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
13:34:00,77 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
13:34:00,180 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
13:34:00,257 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
13:34:00,427 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8041821930000879. input_tokens=961, output_tokens=0
13:34:00,480 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.861238471001343. input_tokens=800, output_tokens=0
13:34:00,617 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9951427609994425. input_tokens=360, output_tokens=0
13:34:00,751 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1302406770009839. input_tokens=779, output_tokens=0
13:34:00,821 datashaper.workflow.workflow INFO executing verb drop
13:34:00,844 datashaper.workflow.workflow INFO executing verb filter
13:34:00,866 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
13:34:01,170 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
13:34:01,171 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
13:34:01,204 datashaper.workflow.workflow INFO executing verb layout_graph
13:34:01,234 datashaper.workflow.workflow INFO executing verb unpack_graph
13:34:01,257 datashaper.workflow.workflow INFO executing verb unpack_graph
13:34:01,280 datashaper.workflow.workflow INFO executing verb drop
13:34:01,313 datashaper.workflow.workflow INFO executing verb filter
13:34:01,350 datashaper.workflow.workflow INFO executing verb select
13:34:01,367 datashaper.workflow.workflow INFO executing verb rename
13:34:01,384 datashaper.workflow.workflow INFO executing verb convert
13:34:01,441 datashaper.workflow.workflow INFO executing verb join
13:34:01,467 datashaper.workflow.workflow INFO executing verb rename
13:34:01,469 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
13:34:01,703 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
13:34:01,703 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
13:34:01,747 datashaper.workflow.workflow INFO executing verb create_final_communities
13:34:01,769 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
13:34:01,963 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
13:34:01,963 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
13:34:01,968 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
13:34:02,7 datashaper.workflow.workflow INFO executing verb create_final_relationships_pre_embedding
13:34:02,31 datashaper.workflow.workflow INFO executing verb create_final_relationships_post_embedding
13:34:02,38 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
13:34:02,254 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_final_entities', 'create_base_text_units', 'create_final_covariates']
13:34:02,255 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
13:34:02,259 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
13:34:02,266 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
13:34:02,270 graphrag.utils.storage INFO read table from storage: create_final_covariates.parquet
13:34:02,315 datashaper.workflow.workflow INFO executing verb create_final_text_units_pre_embedding
13:34:02,379 datashaper.workflow.workflow INFO executing verb select
13:34:02,381 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
13:34:02,578 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes', 'create_final_covariates']
13:34:02,579 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
13:34:02,583 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
13:34:02,588 graphrag.utils.storage INFO read table from storage: create_final_covariates.parquet
13:34:02,634 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
13:34:02,658 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
13:34:02,680 datashaper.workflow.workflow INFO executing verb prepare_community_reports_claims
13:34:02,704 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
13:34:02,730 datashaper.workflow.workflow INFO executing verb prepare_community_reports
13:34:02,731 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 57
13:34:02,793 datashaper.workflow.workflow INFO executing verb create_community_reports
13:34:03,149 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
13:34:03,151 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
13:34:27,641 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:34:27,644 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 24.835916722999173. input_tokens=2281, output_tokens=694
13:34:29,629 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:34:29,634 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 26.81915584499984. input_tokens=2357, output_tokens=749
13:34:31,414 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:34:31,421 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 28.59945634900032. input_tokens=2763, output_tokens=761
13:34:32,802 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:34:32,809 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 29.97699966000073. input_tokens=2297, output_tokens=821
13:34:32,992 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:34:32,998 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 30.19451752399982. input_tokens=2706, output_tokens=823
13:34:35,558 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:34:35,570 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 30.42002570800105. input_tokens=2162, output_tokens=875
13:34:35,627 datashaper.workflow.workflow INFO executing verb window
13:34:35,630 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
13:34:35,850 graphrag.index.run.workflow INFO dependencies for create_base_documents: ['create_final_text_units']
13:34:35,850 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
13:34:35,896 datashaper.workflow.workflow INFO executing verb unroll
13:34:35,922 datashaper.workflow.workflow INFO executing verb select
13:34:35,944 datashaper.workflow.workflow INFO executing verb rename
13:34:35,966 datashaper.workflow.workflow INFO executing verb join
13:34:35,997 datashaper.workflow.workflow INFO executing verb aggregate_override
13:34:36,21 datashaper.workflow.workflow INFO executing verb join
13:34:36,50 datashaper.workflow.workflow INFO executing verb rename
13:34:36,75 datashaper.workflow.workflow INFO executing verb convert
13:34:36,106 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
13:34:36,300 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_base_documents']
13:34:36,301 graphrag.utils.storage INFO read table from storage: create_base_documents.parquet
13:34:36,348 datashaper.workflow.workflow INFO executing verb rename
13:34:36,350 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
13:34:36,432 graphrag.index.cli INFO All workflows completed successfully.
